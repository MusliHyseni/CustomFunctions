{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODWepU1TsLce6fNXr9PT93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MusliHyseni/CustomFunctions/blob/main/CustomFunctions_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEr5HKr61Uo5"
      },
      "outputs": [],
      "source": [
        "import torch, requests, zipfile, os, random, numpy as np, matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "# Create a custom function, which does the ImageFolder's job\n",
        "\"\"\"\n",
        "1. Subclass torch.utils.data.Dataset.\n",
        "\n",
        "2. Initialize our subclass with a targ_dir parameter (the target data directory)\n",
        " and transform parameter (so we have the option to transform our data if needed).\n",
        "\n",
        "3. Create several attributes for paths (the paths of our target images),\n",
        "transform (the transforms we might like to use, this can be None), classes and class_to_idx (from our find_classes() function).\n",
        "\n",
        "4. Create a function to load images from file and return them, this could be using PIL or torchvision.io (for input/output of vision data).\n",
        "\n",
        "5. Overwrite the __len__ method of torch.utils.data.Dataset to return the number of samples in the Dataset,\n",
        "this is recommended but not required. This is so you can call len(Dataset).\n",
        "\n",
        "6. Overwrite the __getitem__ method of torch.utils.data.Dataset to return a single sample from the Dataset, this is required.\n",
        "\"\"\"\n",
        "# This is what function at lines 88-97 does\n",
        "class ImageFolderCustom(Dataset):\n",
        "    def __init__(self, target_dir: str, tranform: None) -> None:\n",
        "        self.paths = list(Path(target_dir).glob(\"*/*.jpg\"))\n",
        "        self.transform = transforms\n",
        "        self.classes, self.class_to_index = find_classes(target_dir)\n",
        "\n",
        "    def load_image(self, index: int) -> Image.Image:\n",
        "        \"Opens an image via a path and returns it.\"\n",
        "        image_path = self.paths[index]\n",
        "        return Image.open(image_path)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.paths)\n",
        "\n",
        "def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "    image = self.load_image(index)\n",
        "    class_name = self.paths[index].parent.name\n",
        "    class_index = self.class_to_index[class_name]\n",
        "\n",
        "    if self.transform:\n",
        "        return self.transform(img), class_index\n",
        "    else:\n",
        "        return img, class_index\n",
        "\n",
        "\n",
        "# Augment train data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Don't augment test data, only reshape\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "custom_train_data = ImageFolderCustom(train_dir, train_transforms)\n",
        "custom_test_data = ImageFolderCustom(test_dir, test_transforms)\n",
        "print(custom_train_data, custom_test_data)\n",
        "\n",
        "\n",
        "\n",
        "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
        "                          classes: List[str] = None,\n",
        "                          n: int = 10,\n",
        "                          display_shape: bool =True,\n",
        "                          seed: int = None):\n",
        "    if n > 10:\n",
        "        n = 10\n",
        "        display_shape = False\n",
        "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
        "\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    random_index = random.sample(range(len(dataset)), k=n)\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    for i, target_sample in enumerate(random_index):\n",
        "        target_image, target_label = dataset[target_sample][0], dataset[target_sample][1]\n",
        "\n",
        "        target_image_adjust = target_image.permute(1, 2, 0)\n",
        "\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(target_image_adjust)\n",
        "        plt.axis(\"off\")\n",
        "        if classes:\n",
        "            title = f\"Class: {classes[target_label]}\"\n",
        "            if display_shape:\n",
        "                title += f\"\\tShape: {target_image_adjust.shape}\"\n",
        "        plt.title(title)\n",
        "\n",
        "# Display random images from ImageFolder created Dataset\n",
        "display_random_images(train_data,\n",
        "                      n=5,\n",
        "                      classes=class_names,\n",
        "                      seed=None)\n"
      ]
    }
  ]
}